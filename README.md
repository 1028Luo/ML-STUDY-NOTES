# Study-Notes

Content included:

1. Supervised Learning:
   
     1.1 Classification:
   
       1.1.1 Logistic regression
   
       1.1.2 Random forest
   
   KNN (K Nearest Neighbor)
   
   SVM
   
   Neural Networks
   
   Decision Trees
   
     1.2 Regression(like classification in continuous space):
       1.2.1 Linear Regression
       Random Forest Regression
       Decision Trees Regression
   SVM, Neural Networks

Ensemble methods:
Ensemble methods generate prediction from a group of models
a. bagging ensemble methods: random forest, trains multiple trees in parallel and the prediction is their average(continous) or most-voted(discrete)
b. boosting ensemble methods: XG Boost, train F0 and get error between F0 and Y
train a new model h1 so that error(F0 + h1 = F1, Y) is lower
iterate this step to reduce error(F1, Y) by training error(F1 + h2 = F2, Y)


   
2. Unsupervised Learning	
  Clustering:
  
  Anomaly Detection,
  K Clustering
Dimensionality Reduction	K-Means, PCA, DBSCAN

3. Reinforcement Learning:
  Game AI, Robotics	Q-Learning, PPO, DQN

4. Generative Models:
  AI-generated text/images	GANs, VAEs, Transformers

5. Data processing

9. other
Recommendation Systems:
  Netflix, Amazon Suggestions	Collaborative Filtering, Deep Learning
NLP & Vision:
  Chatbots, Object Detection	BERT, GPT, CNNs, Vision Transformers


TO DO:
1. Finish chatbot 2.20
   
2. Some more ML learning and summary 2.23

3. Refine resume and start applying 2.25

4. transformer from scratch 2.28

5. Leetcode, 2.25 onwards
