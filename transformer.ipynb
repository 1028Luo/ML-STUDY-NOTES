{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOZF1akLDNetGSb6P0p1M22"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcGyDhXzsals"
      },
      "outputs": [],
      "source": [
        "# Transformer implementation from scratch with PyTorch\n",
        "# https://medium.com/towards-data-science/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-eEKlrqFsi3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Head Attention\n"
      ],
      "metadata": {
        "id": "r3G0IqdhsxnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.module):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "\n",
        "    # super(child_class).some_function(): accessing some function in the parent class\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "    assert d_model % num_heads == 0, \"d_model must be divisble by num heads\"\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.d_k = d_model // num_heads # dimension of each head\n",
        "\n",
        "    # learnable parameters\n",
        "    self.W_q = nn.Linear(d_model, d_model) # projects input embeddings to query vector\n",
        "    self.W_k = nn.Linear(d_model, d_model) # key vector\n",
        "    self.W_v = nn.Linear(d_model, d_model) # value vector\n",
        "    self.W_o = nn.Linear(d_model, d_model) #\n",
        "\n",
        "  def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "\n",
        "    # Q x K / sqrt(dim_k)\n",
        "    attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "      # replace with a very small value so it is ignored by softmax\n",
        "      attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    attn_probs = torch.softmax(attn_scores, dim = -1)\n",
        "    output = torch.matmul(attn_probs, V) # multiply with V\n",
        "\n",
        "    return output\n",
        "\n",
        "  # split x, dim(x) = d_model = d_k * num_heads\n",
        "  def split_heads(self, x):\n",
        "    batch_size, seq_length, d_model = x.size()\n",
        "    # x.view() reshapes x\n",
        "    # transpose to swap the 2nd and 3rd dimensions\n",
        "    return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1,2)\n",
        "\n",
        "  # combining the results of each head into a single tensor\n",
        "  # .contiguous(): ensures that the tensor is stored in a contiguous block of memory.\n",
        "  # This is necessary before calling .view() because transpose() can sometimes return a non-contiguous tensor.\n",
        "  def combine_heads(self, x):\n",
        "    batch_size, _, seq_length, d_k = x.size()\n",
        "    return x.transpose(1,2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "  def forward(self, Q, K, V, mask = None):\n",
        "\n",
        "    # transforms Query Q into the space of d_model and then split into heads\n",
        "    Q = self.split_heads(self.W_q(Q))\n",
        "    K = self.split_heads(self.W_q(K))\n",
        "    V = self.split_heads(self.W_q(V))\n",
        "\n",
        "    attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "    output = self.W_o(self.combine_heads(attn_output))\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "rEghBGycs1si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (FNN) Position-wise Feed-Forward Networks"
      ],
      "metadata": {
        "id": "gEvpRDk5drhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FNN is what follows attention\n",
        "# also where \"facts\" are stored in the model\n",
        "# a very good explanation of FNN, model # parameters and the scaling law\n",
        "# https://www.youtube.com/watch?v=9-Jl0dxWQs8\n",
        "\n",
        "#\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "5TA0_249dvmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder"
      ],
      "metadata": {
        "id": "ErYGBrO52i7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder is essentailly attention + FNN\n",
        "# it is called encoder because it encodes imformation into the model"
      ],
      "metadata": {
        "id": "hb8K6TnM2l2r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}